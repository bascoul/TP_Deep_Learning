{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Part1_TensorFlow.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "WBk0ZDWY-ff8"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bascoul/TP_Deep_Learning/blob/master/Part1_TensorFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBk0ZDWY-ff8",
        "colab_type": "text"
      },
      "source": [
        "<table align=\"center\">\n",
        "  <td align=\"center\"><a target=\"_blank\" href=\"http://introtodeeplearning.com\">\n",
        "        <img src=\"http://introtodeeplearning.com/images/colab/mit.png\" style=\"padding-bottom:5px;\" />\n",
        "      Visit MIT Deep Learning</a></td>\n",
        "  <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/aamini/introtodeeplearning/blob/master/lab1/solutions/Part1_TensorFlow_Solution.ipynb\">\n",
        "        <img src=\"http://introtodeeplearning.com/images/colab/colab.png?v2.0\"  style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n",
        "  <td align=\"center\"><a target=\"_blank\" href=\"https://github.com/aamini/introtodeeplearning/blob/master/lab1/solutions/Part1_TensorFlow_Solution.ipynb\">\n",
        "        <img src=\"http://introtodeeplearning.com/images/colab/github.png\"  height=\"70px\" style=\"padding-bottom:5px;\"  />View Source on GitHub</a></td>\n",
        "</table>\n",
        "\n",
        "# Copyright Information\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eI6DUic-6jo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Copyright 2020 MIT 6.S191 Introduction to Deep Learning. All Rights Reserved.\n",
        "# \n",
        "# Licensed under the MIT License. You may not use this file except in compliance\n",
        "# with the License. Use and/or modification of this code outside of 6.S191 must\n",
        "# reference:\n",
        "#\n",
        "# © MIT 6.S191: Introduction to Deep Learning\n",
        "# http://introtodeeplearning.com\n",
        "#"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57knM8jrYZ2t",
        "colab_type": "text"
      },
      "source": [
        "# Lab 1: Introduction à TensorFlow et à la génération de musique with RNNs\n",
        "\n",
        "Dans ce laboratoire, vous vous familiariserez avec l'utilisation de TensorFlow et apprendrez comment l'utiliser pour résoudre des tâches d'apprentissage approfondi. Passez en revue le code et exécutez chaque cellule. En cours de route, vous rencontrerez plusieurs blocs A FAIRE - suivez les instructions pour les remplir avant de lancer ces cellules et de continuer.\n",
        "\n",
        "\n",
        "# Part 1: Introduction à TensorFlow\n",
        "\n",
        "## 0.1 Installer TensorFlow\n",
        "\n",
        "TensorFlow est une bibliothèque de logiciels largement utilisée dans l'apprentissage machine. Nous apprendrons ici comment les calculs sont représentés et comment définir un simple réseau de neurones dans TensorFlow. Pour tous les laboratoires de 6.S191 2020, nous utiliserons la dernière version de TensorFlow, TensorFlow 2, qui offre une grande flexibilité et la possibilité d'exécuter impérativement des opérations, tout comme en Python. Vous remarquerez que TensorFlow 2 est assez similaire à Python dans sa syntaxe et son exécution impérative. Installons TensorFlow et quelques dépendances.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkaimNJfYZ2w",
        "colab_type": "code",
        "outputId": "91e4a518-436f-4606-d422-3faff02e85b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "\n",
        "# On charge et on importe le package MIT 6.S191\n",
        "!pip install mitdeeplearning\n",
        "import mitdeeplearning as mdl\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n",
            "Collecting mitdeeplearning\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8b/3b/b9174b68dc10832356d02a2d83a64b43a24f1762c172754407d22fc8f960/mitdeeplearning-0.1.2.tar.gz (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 6.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from mitdeeplearning) (1.18.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from mitdeeplearning) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from mitdeeplearning) (4.38.0)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from mitdeeplearning) (0.17.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym->mitdeeplearning) (1.12.0)\n",
            "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->mitdeeplearning) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym->mitdeeplearning) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym->mitdeeplearning) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->mitdeeplearning) (0.16.0)\n",
            "Building wheels for collected packages: mitdeeplearning\n",
            "  Building wheel for mitdeeplearning (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mitdeeplearning: filename=mitdeeplearning-0.1.2-cp36-none-any.whl size=2114586 sha256=18ae236f982167794905fa533c4a1427c1d8c13ad054ae908e42b95b07d6ae81\n",
            "  Stored in directory: /root/.cache/pip/wheels/27/e1/73/5f01c787621d8a3c857f59876c79e304b9b64db9ff5bd61b74\n",
            "Successfully built mitdeeplearning\n",
            "Installing collected packages: mitdeeplearning\n",
            "Successfully installed mitdeeplearning-0.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QNMcdP4m3Vs",
        "colab_type": "text"
      },
      "source": [
        "## 1.1 Pourquoi TensorFlow se nomme TensorFlow ?\n",
        "\n",
        "TensorFlow est appelé \"TensorFlow\" parce qu'il gère le flux (nœud/opération mathématique) des tenseurs, qui sont des structures de données que l'on peut considérer comme des tableaux multidimensionnels. Les tenseurs sont représentés sous forme de tableaux à n dimensions de types de données de base tels qu'une chaîne ou un entier. Ils permettent de généraliser les vecteurs et les matrices à des dimensions plus élevées.\n",
        "\n",
        "La \"forme\" d'un tenseur définit son nombre de dimensions et la taille de chaque dimension. Le \"rang\" d'un tenseur correspond au nombre de dimensions (n-dimensions) -- on peut aussi considérer qu'il s'agit de l'ordre ou du degré du tenseur.\n",
        "\n",
        "Examinons d'abord les tenseurs 0-d, dont un scalaire est un exemple :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFxztZQInlAB",
        "colab_type": "code",
        "outputId": "e3239a50-8e4e-4c6b-9220-feddc86963dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "sport = tf.constant(\"Tennis\", tf.string)\n",
        "number = tf.constant(1.41421356237, tf.float64)\n",
        "\n",
        "print(\"`sport` is a {}-d Tensor\".format(tf.rank(sport).numpy()))\n",
        "print(\"`number` is a {}-d Tensor\".format(tf.rank(number).numpy()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`sport` is a 0-d Tensor\n",
            "`number` is a 0-d Tensor\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dljcPUcoJZ6",
        "colab_type": "text"
      },
      "source": [
        "Les vecteurs et les listes peuvent être utilisés pour créer des tenseurs 1-d :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaHXABe8oPcO",
        "colab_type": "code",
        "outputId": "39c7f4f1-a9b2-4a75-b8aa-bea87cd9151a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "sports = tf.constant([\"Tennis\", \"Basketball\"], tf.string)\n",
        "numbers = tf.constant([3.141592, 1.414213, 2.71821], tf.float64)\n",
        "\n",
        "print(\"`sports` is a {}-d Tensor with shape: {}\".format(tf.rank(sports).numpy(), tf.shape(sports)))\n",
        "print(\"`numbers` is a {}-d Tensor with shape: {}\".format(tf.rank(numbers).numpy(), tf.shape(numbers)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`sports` is a 1-d Tensor with shape: [2]\n",
            "`numbers` is a 1-d Tensor with shape: [3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvffwkvtodLP",
        "colab_type": "text"
      },
      "source": [
        "Ensuite, nous envisageons de créer des tenseurs 2-d (c'est-à-dire des matrices) et des tenseurs de rang supérieur. Par exemple, dans les futurs laboratoires de traitement d'images et de vision par ordinateur, nous utiliserons des tenseurs 4-d. Ici, les dimensions correspondent au nombre d'images d'exemple dans notre lot, à la hauteur et à la largeur de l'image, ainsi qu'au nombre de canaux de couleur."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFeBBe1IouS3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Définir des tenseurs d'ordre supérieur ###\n",
        "\n",
        "'''A FAIRE: Définir un tenseur 2-d'''\n",
        "\n",
        "# matrix = # A FAIRE\n",
        "\n",
        "assert isinstance(matrix, tf.Tensor), \"matrix doit être un objet Tensor de tf\"\n",
        "assert tf.rank(matrix).numpy() == 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zv1fTn_Ya_cz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''A FAIRE: Définir un tenseur 4-d.'''\n",
        "# Utiliser tf.zeros pour initialiser un tenseur 4d avec des zéros et d'une taille 10 x 256 x 256 x 3.  \n",
        "#   On peut considérer qu'il s'agit de 10 images où chaque image est en RVB 256 x 256.\n",
        "\n",
        "# images = # A FAIRE\n",
        "\n",
        "assert isinstance(images, tf.Tensor), \"matrix must be a tf Tensor object\"\n",
        "assert tf.rank(images).numpy() == 4, \"matrix must be of rank 4\"\n",
        "assert tf.shape(images).numpy().tolist() == [10, 256, 256, 3], \"matrix is incorrect shape\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkaCDOGapMyl",
        "colab_type": "text"
      },
      "source": [
        "Comme vous l'avez vu, la \"forme\" d'un tenseur fournit le nombre d'éléments dans chaque dimension du tenseur. La \"forme\" est très utile, et nous l'utiliserons souvent. Vous pouvez également utiliser le découpage pour accéder aux sous-tenseurs d'un tenseur de rang supérieur :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhaufyObuLEG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "row_vector = matrix[1]\n",
        "column_vector = matrix[:,2]\n",
        "scalar = matrix[1, 2]\n",
        "\n",
        "print(\"`row_vector`: {}\".format(row_vector.numpy()))\n",
        "print(\"`column_vector`: {}\".format(column_vector.numpy()))\n",
        "print(\"`scalar`: {}\".format(scalar.numpy()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iD3VO-LZYZ2z",
        "colab_type": "text"
      },
      "source": [
        "## 1.2 Calculs sur les tenseurs\n",
        "\n",
        "Une façon pratique d'envisager et de visualiser les calculs dans TensorFlow est de les faire sous forme de graphiques. Nous pouvons définir ce graphique en termes de Tenseurs, qui contiennent des données, et les opérations mathématiques qui agissent sur ces Tenseurs dans un certain ordre. Prenons un exemple simple, et définissons ce calcul en utilisant TensorFlow :\n",
        "\n",
        "![alt text](https://raw.githubusercontent.com/aamini/introtodeeplearning/master/lab1/img/add-graph.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_YJrZsxYZ2z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Créer les noeuds dans le graphe et initialiser les valeurs\n",
        "a = tf.constant(15)\n",
        "b = tf.constant(61)\n",
        "\n",
        "# On les ajoute !\n",
        "c1 = tf.add(a,b)\n",
        "c2 = a + b # TensorFlow surcharge l'opération \"+\" pour qu'elle puisse être utilisée sur les tenseurs\n",
        "print(c1)\n",
        "print(c2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mbfv_QOiYZ23",
        "colab_type": "text"
      },
      "source": [
        "Remarquez comment nous avons créé un graphique de calcul composé d'opérations TensorFlow, et comment la sortie est un Tensor avec la valeur 76 -- nous venons de créer un graphique de calcul composé d'opérations, et il les a exécutées et nous a rendu le résultat.\n",
        "\n",
        "Examinons maintenant un exemple un peu plus compliqué :\n",
        "\n",
        "![alt text](https://raw.githubusercontent.com/aamini/introtodeeplearning/master/lab1/img/computation-graph.png)\n",
        "\n",
        "Ici, nous prenons deux entrées, a, b, et calculons une sortie e. Chaque noeud du graphique représente une opération qui prend une entrée, fait un calcul, et passe sa sortie à un autre noeud.\n",
        "\n",
        "Définissons une fonction simple dans TensorFlow pour construire cette fonction de calcul :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "PJnfzpWyYZ23",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Définir des calculs de tenseurs ###\n",
        "\n",
        "# Construire une fonction simple de calcul\n",
        "def func(a,b):\n",
        "  '''A FAIRE: Définir l'opération pour c, d, e (utiliser tf.add, tf.subtract, tf.multiply).'''\n",
        "\n",
        "  # c = # TODO\n",
        "\n",
        "  # d = # TODO\n",
        "\n",
        "  # e = # TODO\n",
        "  return e"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwrRfDMS2-oy",
        "colab_type": "text"
      },
      "source": [
        "Maintenant, nous pouvons appeler cette fonction pour exécuter le graphique de calcul étant donné certaines entrées \"a,b\" :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnwsf8w2uF7p",
        "colab_type": "code",
        "outputId": "d8bb7389-8049-45f7-aa87-8be9b0d8ca5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Considérons les exemples de valeur pour a,b\n",
        "a, b = 1.5, 2.5\n",
        "# Exécuter le calcul\n",
        "e_out = func(a,b)\n",
        "print(e_out)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(6.0, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HqgUIUhYZ29",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Remarquez que notre sortie est un tenseur dont la valeur est définie par la sortie du calcul, et que la sortie n'a pas de forme car il s'agit d'une valeur scalaire unique."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1h4o9Bb0YZ29",
        "colab_type": "text"
      },
      "source": [
        "## 1.3 Réseaux de neurones dans TensorFlow\n",
        "Nous pouvons également définir des réseaux de neurones dans TensorFlow. TensorFlow utilise une API de haut niveau appelée [Keras](https://www.tensorflow.org/guide/keras) qui fournit un cadre puissant et intuitif pour la construction et la formation de modèles d'apprentissage profond.\n",
        "\n",
        "Considérons d'abord l'exemple d'un perceptron simple défini par une seule couche dense : $ y = \\sigma(Wx + b)$, où $W$ représente une matrice de poids, $b$ est un biais, $x$ est l'entrée, $\\sigma$ est la fonction d'activation sigmoïde, et $y$ est la sortie. Nous pouvons également visualiser cette opération à l'aide d'un graphique : \n",
        " \n",
        "\n",
        "![alt text](https://raw.githubusercontent.com/aamini/introtodeeplearning/master/lab1/img/computation-graph-2.png)\n",
        "\n",
        "Les tenseurs peuvent s'écouler à travers des types abstraits appelés [``Layers``] (https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer) -- les éléments constitutifs des réseaux de neurones. Les \"couches\" mettent en œuvre des opérations communes de réseaux de neurones, et sont utilisées pour mettre à jour les poids, calculer les pertes, et définir la connectivité entre les couches. Nous allons d'abord définir une \"couche\" pour mettre en œuvre le perceptron simple défini ci-dessus.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HutbJk-1kHPh",
        "colab_type": "code",
        "outputId": "e44c9f85-fcf2-496a-ca02-a10e25214719",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "### Définir un calque de réseau ###\n",
        "\n",
        "# n_output_nodes: nombre de noeuds de sortie\n",
        "# input_shape: forme de l'entrée\n",
        "# x: entrée de la couche\n",
        "\n",
        "# La classe OurDenseLayer hérite de la classe tf.keras.layers.Layer\n",
        "class OurDenseLayer(tf.keras.layers.Layer):\n",
        "  # Un constructeur est défini passant le nombre de noeuds de sortie en paramètre\n",
        "  def __init__(self, n_output_nodes):\n",
        "    # On lance le constructeur de la classe mère tf.keras.layers.Layer\n",
        "    super(OurDenseLayer, self).__init__()\n",
        "    self.n_output_nodes = n_output_nodes\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    # Rappel dans un tableau -1 est le dernier élément de la liste\n",
        "    d = int(input_shape[-1])\n",
        "    # Définir et intialiser les paramètres: une matrice de poids W et un biais b\n",
        "    # Noter que l'initialisation des paramètres est aléatoire !\n",
        "    self.W = self.add_weight(\"weight\", shape=[d, self.n_output_nodes]) # noter la dimension\n",
        "    self.b = self.add_weight(\"bias\", shape=[1, self.n_output_nodes]) # noter la dimension\n",
        "\n",
        "  def call(self, x):\n",
        "    '''A FAIRE: définir l'opération pour z (utiliser tf.matmul)'''\n",
        "    # Attention à l'ordre dans la multiplication de W et x\n",
        "\n",
        "    # z = # A FAIRE\n",
        "\n",
        "    '''A FAIRE: définir l'opération pour la sortie (utiliser tf.sigmoid)'''\n",
        "\n",
        "    # y = # A FAIRE\n",
        "    return y\n",
        "\n",
        "# Comme les paramètres des couches sont initialisés de manière aléatoire ?\n",
        "# Nous allons définir une graine aléatoire pour la reproductibilité\n",
        "tf.random.set_seed(1)\n",
        "# On met en place une couche dense avec 3 noeuds de sortie \n",
        "layer = OurDenseLayer(3)\n",
        "# On construit les tenseurs W et b à partir d'une entrée à 2 noeuds\n",
        "# La tenseur de l'entrée est un tenseur d'une ligne avec 2 valeurs\n",
        "# ce qui correspond à un tenseur de forme de forme (1, 2)\n",
        "layer.build((1,2))\n",
        "# On définit le tenseur d'entrée en définissant ses valeurs\n",
        "x_input = tf.constant([[1,2.]], shape=(1,2))\n",
        "y = layer.call(x_input)\n",
        "\n",
        "# tester la sortie!\n",
        "#La commande numpy() permet de transformer un tenseur en tableau numpy\n",
        "print(y.numpy())\n",
        "mdl.lab1.test_custom_dense_layer_output(y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.26978594 0.45750412 0.66536945]]\n",
            "[PASS] test_custom_dense_layer_output\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jt1FgM7qYZ3D",
        "colab_type": "text"
      },
      "source": [
        "De manière pratique, TensorFlow a défini un certain nombre de \"couches\" qui sont couramment utilisées dans les réseaux neuronaux, par exemple une Dense] (https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense?version=stable). Maintenant, au lieu d'utiliser une seule \"couche\" pour définir notre simple réseau de neurones, nous allons utiliser le modèle [`Sequential de Keras et une seule couche Dense pour définir notre réseau. Avec l'API \"séquentielle\", vous pouvez facilement créer des réseaux de neurones en empilant des couches comme des blocs de construction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WXTpmoL6TDz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Définir un réseau de neurones en utilisant l'API Sequential ###\n",
        "\n",
        "# Importer les packages utiles\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Définir le nombre de sortie\n",
        "n_output_nodes = 3\n",
        "\n",
        "# Premièrement définir le modèle \n",
        "model = Sequential()\n",
        "\n",
        "'''A FAIRE: Définir une couche dense (totalement connectée) pour calculer z'''\n",
        "# Rappel: les couches denses sont définies par les paramètres W et b!\n",
        "# Vous pouvez lire plus sur l'initialisation de W et b dans la documentation de TF:) \n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense?version=stable\n",
        "# Vous devez définir dans la fonction Dense, le nombre de noeuds en sortie ainsi que la fonction d'activation\n",
        "\n",
        "# dense_layer = # A FAIRE\n",
        "\n",
        "# Ajouter la couche dense au modèle\n",
        "model.add(dense_layer)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDGcwYfUyR-U",
        "colab_type": "text"
      },
      "source": [
        "C'est ça ! Nous avons défini notre modèle en utilisant l'API séquentielle. Maintenant, nous pouvons le tester en utilisant un exemple d'entrée :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sg23OczByRDb",
        "colab_type": "code",
        "outputId": "d7fed5fa-c1e5-4774-e9b9-42f46863232d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Tester le modèle avec un exemple d'entrée\n",
        "x_input = tf.constant([[1,2.]], shape=(1,2))\n",
        "\n",
        "'''A FAIRE: alimenter le modèle et prévoir le résultat!'''\n",
        "# Alimenter le modèle consiste à lui passer en paramètre l'entrée\n",
        "\n",
        "# model_output = # A FAIRE\n",
        "print(model_output)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.5607363 0.6566898 0.1249697]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "596NvsOOtr9F",
        "colab_type": "text"
      },
      "source": [
        "En plus de définir des modèles à l'aide de l'API \"séquentielle\", nous pouvons également définir des réseaux de neurones en sous-classant directement la classe [`Model`] (https://https://www.tensorflow.org/api_docs/python/tf/keras/Model?version=stable), qui regroupe des couches pour permettre l'apprentissage et l'inférence de modèles. La classe `Model` capture ce que nous appelons un \"modèle\" ou un \"réseau\". En utilisant la sous-classification, nous pouvons créer une classe pour notre modèle, puis définir le passage à travers le réseau en utilisant la fonction \"call\". Le sous-classement offre la flexibilité de définir des couches personnalisées, des boucles d'entraînement personnalisées, des fonctions d'activation personnalisées et des modèles personnalisés. Définissons maintenant le même réseau de neurones que ci-dessus en utilisant le sous-classement plutôt que le modèle \"séquentiel\".\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4aCflPVyViD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Définir un modèle en utilisant le sous-classement ###\n",
        "\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "class SubclassModel(tf.keras.Model):\n",
        "\n",
        "  # Dans __init__, nous définissons les calques du modèle\n",
        "  def __init__(self, n_output_nodes):\n",
        "    super(SubclassModel, self).__init__()\n",
        "    '''A FAIRE: Notre modèle consiste en un seul calque Dense. Définir ce calque.''' \n",
        "\n",
        "    # self.dense_layer = '''A FAIRE: Calque Dense'''\n",
        "\n",
        "  # Dans la fonction call, nous définissons le passage en avant du modèle.\n",
        "  def call(self, inputs):\n",
        "    return self.dense_layer(inputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0-lwHDk4irB",
        "colab_type": "text"
      },
      "source": [
        "Tout comme le modèle que nous avons construit en utilisant l'API \"séquentielle\", nous allons tester notre \"SubclassModel\" en utilisant un exemple d'entrée.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhB34RA-4gXb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_output_nodes = 3\n",
        "model = SubclassModel(n_output_nodes)\n",
        "\n",
        "x_input = tf.constant([[1,2.]], shape=(1,2))\n",
        "\n",
        "print(model.call(x_input))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTIFMJLAzsyE",
        "colab_type": "text"
      },
      "source": [
        "Il est important de noter que le sous-classement nous offre une grande flexibilité pour définir des modèles personnalisés. Par exemple, nous pouvons utiliser des arguments booléens dans la fonction \"appel\" pour spécifier différents comportements du réseau, par exemple différents comportements pendant la formation et l'inférence. Supposons que dans certains cas nous voulions que notre réseau produise simplement l'entrée, sans aucune perturbation. Nous définissons un argument booléen \"isidentity\" pour contrôler ce comportement :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7jzGX5D1xT5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Définir un modèle en utilisant des sous-classes et en spécifiant un comportement personnalisé ###\n",
        "\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "class IdentityModel(tf.keras.Model):\n",
        "\n",
        "  # Comme précédemment, dans __init__ nous définissons les couches du modèle\n",
        "  # Puisque notre comportement souhaité implique le passage en avant, cette partie est inchangée\n",
        "  def __init__(self, n_output_nodes):\n",
        "    super(IdentityModel, self).__init__()\n",
        "    self.dense_layer = tf.keras.layers.Dense(n_output_nodes, activation='sigmoid')\n",
        "\n",
        "  '''A FAIRE: Mettre en œuvre le comportement selon lequel le réseau produit l'entrée, inchangée, sous le contrôle de l'argument isidentity.'''\n",
        "  def call(self, inputs, isidentity=False):\n",
        "    # Il faut commencer à évaluer le résultat de la couche dense quand on lui passe inputs\n",
        "    x =  # A FAIRE\n",
        "    # Ensuite si isidenty est vrai on retourne inputs sinon on retourne x\n",
        "    # A FAIRE\n",
        "    # A FAIRE\n",
        "    return x\n",
        "  \n",
        "  # def call(self, inputs, isidentity=False):\n",
        "    # A FAIRE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ku4rcCGx5T3y",
        "colab_type": "text"
      },
      "source": [
        "Testons ce comportement :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzC0mgbk5dp2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_output_nodes = 3\n",
        "model = IdentityModel(n_output_nodes)\n",
        "\n",
        "x_input = tf.constant([[1,2.]], shape=(1,2))\n",
        "'''A FAIRE: passer l'entrée dans le modèle et appeler avec et sans l'option d'identité d'entrée.'''\n",
        "\n",
        "# out_activate = # A FAIRE\n",
        "\n",
        "# out_identity = # A FAIRE\n",
        "\n",
        "print(\"Network output with activation: {}; network identity output: {}\".format(out_activate.numpy(), out_identity.numpy()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7V1dEqdk6VI5",
        "colab_type": "text"
      },
      "source": [
        "Maintenant que nous avons appris à définir les \"couches\" ainsi que les réseaux neuronaux dans TensorFlow en utilisant à la fois les API \"séquentielles\" et de sous-classement, nous sommes prêts à nous intéresser à la manière de mettre en œuvre la formation des réseaux avec rétropropagation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQwDhKn8kbO2",
        "colab_type": "text"
      },
      "source": [
        "## 1.4 Dérivation automatique dans TensorFlow\n",
        "\n",
        "[La dérivation automatique](https://en.wikipedia.org/wiki/Automatic_differentiation)\n",
        "est l'une des parties les plus importantes de TensorFlow et constitue l'épine dorsale de l'apprentissage avec la\n",
        "[backpropagation](https://en.wikipedia.org/wiki/Backpropagation). Nous utiliserons le GradientTape de TensorFlow [`tf.GradientTape`](https://www.tensorflow.org/api_docs/python/tf/GradientTape?version=stable) pour retracer les opérations de calcul des gradients plus tard. \n",
        "\n",
        "Lorsqu'un passage en avant est effectué à travers le réseau, toutes les opérations de passage en avant sont enregistrées sur une \"bande\" ; ensuite, pour calculer le gradient, la bande est lue en arrière. Par défaut, la bande est rejetée après avoir été lue à l'envers ; cela signifie qu'une `tf.GradientTape` particulière ne peut calculer qu'un seul gradient, et les appels suivants provoquent une erreur d'exécution. Cependant, nous pouvons calculer plusieurs gradients sur le même calcul en créant une bande de gradients \"persistante\". \n",
        "\n",
        "Tout d'abord, nous examinerons comment calculer les gradients à l'aide de GradientTape et y accéder pour le calcul. Nous définissons la fonction simple $ y = x^2$ et calculons le gradient :\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdkqk8pw5yJM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Calcul du Gradient avec GradientTape ###\n",
        "\n",
        "# y = x^2\n",
        "# Exemple: x = 3.0\n",
        "x = tf.Variable(3.0)\n",
        "\n",
        "# Initialiser l'enregistrement du gradient\n",
        "with tf.GradientTape() as tape:\n",
        "  # Définir la fonction\n",
        "  y = x * x\n",
        "# Accéder au gradient -- dériver y par rapport à x\n",
        "dy_dx = tape.gradient(y, x)\n",
        "\n",
        "assert dy_dx.numpy() == 6.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhU5metS5xF3",
        "colab_type": "text"
      },
      "source": [
        "Dans l'entraînement des réseaux de neurones, nous utilisons la différenciation et la descente stochastique de gradient (SGD) pour optimiser une fonction de perte. Maintenant que nous avons une idée de la façon dont la \"bande de gradient\" peut être utilisée pour calculer et accéder aux dérivés, nous allons examiner un exemple où nous utilisons la différenciation automatique et la SGD pour trouver le minimum de $L=(x-x_f)^2$. Ici, $x_f$ est une variable pour une valeur souhaitée que nous essayons d'optimiser ; $L$ représente une perte que nous essayons de minimiser. Bien que nous puissions clairement résoudre ce problème de manière analytique ($x_{min}=x_f$), le fait de considérer comment nous pouvons le calculer en utilisant `GradientTape` nous prépare pour les futurs laboratoires où nous utiliserons la descente de gradient pour optimiser les pertes de réseaux neuronaux entiers.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [
            "py"
          ],
          "id": ""
        },
        "colab_type": "code",
        "id": "7g1yWiSXqEf-",
        "colab": {}
      },
      "source": [
        "### Minimisation de la fonction avec la différentiation automatique et le SGD ###\n",
        "\n",
        "# Initialiser une variable aléatoire pour la valeur initiale de x\n",
        "x = tf.Variable([tf.random.normal([1])])\n",
        "print(\"Initializing x={}\".format(x.numpy()))\n",
        "\n",
        "learning_rate = 1e-2 # learning rate pour SGD\n",
        "history = []\n",
        "# Définir la valeur cible\n",
        "x_f = 4\n",
        "\n",
        "# Nous dirigerons le SGD pendant un certain nombre d'itérations. À chaque itération, nous calculons la perte,\n",
        "#   calculons la dérivée de la perte par rapport à x, et effectuons la mise à jour du SGD..\n",
        "for i in range(500):\n",
        "  with tf.GradientTape() as tape:\n",
        "    '''A FAIRE: définir la perte comme décrit ci-dessus'''\n",
        "    # En python la fonction puissance de n s'écrit **n\n",
        "\n",
        "    # loss = # A FAIRE\n",
        "\n",
        "  # minimisation de la perte en utilisant l'enregistrement du gradient\n",
        "  grad = tape.gradient(loss, x) # calculer la dérivée de la perte par rapport à x\n",
        "  new_x = x - learning_rate*grad # mise à jour du sgd\n",
        "  x.assign(new_x) # mettre à jour la valeur de x\n",
        "  history.append(x.numpy()[0])\n",
        "\n",
        "# Traçer l'évolution de x au cours de l'optimisation par rapport à x_f!\n",
        "plt.plot(history)\n",
        "plt.plot([0, 500],[x_f,x_f])\n",
        "plt.legend(('Predicted', 'True'))\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('x value')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pC7czCwk3ceH",
        "colab_type": "text"
      },
      "source": [
        "La `GradientTape` fournit un cadre extrêmement souple pour la différenciation automatique. Afin de retracer les erreurs de propagation à travers un réseau de neurones, nous suivons les passages sur la bande, utilisons ces informations pour déterminer les gradients, puis utilisons ces gradients pour l'optimisation en utilisant SGD.\n"
      ]
    }
  ]
}