{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bascoul/TP_Deep_Learning/blob/master/text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DweYe9FcbMK_"
      },
      "source": [
        "##### Copyright 2018 The TensorFlow Authors.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "AVV2e0XKbJeX",
        "colab": {}
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sUtoed20cRJJ"
      },
      "source": [
        "# Charger le texte"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1ap_W4aQcgNT"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/load_data/text\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/load_data/text.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/load_data/text.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/load_data/text.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NWeQAo0Ec_BL"
      },
      "source": [
        "Ce tutoriel fournit un exemple d'utilisation de `tf.data.TextLineDataset` pour charger des exemples à partir de fichiers texte. Le fichier `TextLineDataset` est conçu pour créer un ensemble de données à partir d'un fichier texte, dans lequel chaque exemple est une ligne de texte du fichier original. Ceci est potentiellement utile pour toutes les données textuelles qui sont principalement basées sur des lignes (par exemple, la poésie ou les journaux d'erreurs).\n",
        "\n",
        "Dans ce tutoriel, nous utiliserons trois traductions anglaises différentes du même ouvrage, l'Illiade d'Homère, et nous formerons un modèle pour identifier le traducteur à qui une seule ligne de texte a été donnée."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fgZ9gjmPfSnK"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "baYFZMW_bJHh",
        "outputId": "fd2ef2f9-da05-4d59-c9ad-429b8e917965",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  !pip install tf-nightly\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tf-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a2/74/ad6fe36fc7a93333df3b19f85cd7e943d9dca426f33a9d281ed55a3ccf74/tf_nightly-2.2.0.dev20200323-cp36-cp36m-manylinux2010_x86_64.whl (533.1MB)\n",
            "\u001b[K     |████████████████████████████████| 533.1MB 24kB/s \n",
            "\u001b[?25hCollecting astunparse==1.6.3\n",
            "  Downloading https://files.pythonhosted.org/packages/2b/03/13dde6512ad7b4557eb792fbcf0c653af6076b81e5941d36ec61f7ce6028/astunparse-1.6.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.2.0)\n",
            "Collecting gast==0.3.3\n",
            "  Downloading https://files.pythonhosted.org/packages/d6/84/759f5dd23fec8ba71952d97bcc7e2c9d7d63bdc582421f3cd4be845f0c98/gast-0.3.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.34.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.12.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.24.3)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.4.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.18.2)\n",
            "Collecting tf-estimator-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/92/32e52c6ef1f3db82c3eea5b2db08f4a271d55c586b9e30d58658d628d21f/tf_estimator_nightly-2.3.0.dev2020032401-py2.py3-none-any.whl (455kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 48.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.9.0)\n",
            "Collecting h5py<2.11.0,>=2.10.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/06/cafdd44889200e5438b897388f3075b52a8ef01f28a17366d91de0fa2d05/h5py-2.10.0-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 47.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.2.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.10.0)\n",
            "Collecting tb-nightly<2.3.0a0,>=2.2.0a0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/cd/90edb987326c277e185ca07b1c976459cf83aec9ba859246fca7dc0b893f/tb_nightly-2.2.0a20200323-py3-none-any.whl (2.8MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8MB 51.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tf-nightly) (46.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.3.0a0,>=2.2.0a0->tf-nightly) (3.2.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.3.0a0,>=2.2.0a0->tf-nightly) (1.0.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.3.0a0,>=2.2.0a0->tf-nightly) (0.4.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.3.0a0,>=2.2.0a0->tf-nightly) (1.7.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.3.0a0,>=2.2.0a0->tf-nightly) (2.21.0)\n",
            "Collecting tensorboard-plugin-wit>=1.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/ec/3da49289b93963bd8b32d29ed108f1809436ff3d9cd4e29c90bac4a7292f/tensorboard_plugin_wit-1.6.0.post2-py3-none-any.whl (775kB)\n",
            "\u001b[K     |████████████████████████████████| 778kB 46.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly<2.3.0a0,>=2.2.0a0->tf-nightly) (1.3.0)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly<2.3.0a0,>=2.2.0a0->tf-nightly) (4.0)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly<2.3.0a0,>=2.2.0a0->tf-nightly) (3.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly<2.3.0a0,>=2.2.0a0->tf-nightly) (0.2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<2.3.0a0,>=2.2.0a0->tf-nightly) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<2.3.0a0,>=2.2.0a0->tf-nightly) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<2.3.0a0,>=2.2.0a0->tf-nightly) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<2.3.0a0,>=2.2.0a0->tf-nightly) (2019.11.28)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly<2.3.0a0,>=2.2.0a0->tf-nightly) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tb-nightly<2.3.0a0,>=2.2.0a0->tf-nightly) (0.4.8)\n",
            "\u001b[31mERROR: tensorflow 1.15.0 has requirement gast==0.2.2, but you'll have gast 0.3.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-federated 0.12.0 has requirement tensorflow~=2.1.0, but you'll have tensorflow 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-federated 0.12.0 has requirement tensorflow-addons~=0.7.0, but you'll have tensorflow-addons 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: astunparse, gast, tf-estimator-nightly, h5py, tensorboard-plugin-wit, tb-nightly, tf-nightly\n",
            "  Found existing installation: gast 0.2.2\n",
            "    Uninstalling gast-0.2.2:\n",
            "      Successfully uninstalled gast-0.2.2\n",
            "  Found existing installation: h5py 2.8.0\n",
            "    Uninstalling h5py-2.8.0:\n",
            "      Successfully uninstalled h5py-2.8.0\n",
            "Successfully installed astunparse-1.6.3 gast-0.3.3 h5py-2.10.0 tb-nightly-2.2.0a20200323 tensorboard-plugin-wit-1.6.0.post2 tf-estimator-nightly-2.3.0.dev2020032401 tf-nightly-2.2.0.dev20200323\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YWVWjyIkffau"
      },
      "source": [
        "Les textes des trois traductions sont de :\n",
        "\n",
        " - [William Cowper](https://en.wikipedia.org/wiki/William_Cowper) — [text](https://storage.googleapis.com/download.tensorflow.org/data/illiad/cowper.txt)\n",
        "\n",
        "Achilles sing, O Goddess! Peleus' son;\n",
        "\n",
        "His wrath pernicious, who ten thousand woes\n",
        "\n",
        "Caused to Achaia's host, sent many a soul\n",
        "\n",
        "Illustrious into Ades premature,\n",
        "\n",
        "..... en tout 19143 lignes\n",
        "\n",
        "\n",
        " - [Edward, Earl of Derby](https://en.wikipedia.org/wiki/Edward_Smith-Stanley,_14th_Earl_of_Derby) — [text](https://storage.googleapis.com/download.tensorflow.org/data/illiad/derby.txt)\n",
        "\n",
        "Of Peleus' son, Achilles, sing, O Muse,\n",
        "\n",
        "The vengeance, deep and deadly; whence to Greece\n",
        "\n",
        "Unnumbered ills arose; which many a soul\n",
        "\n",
        "Of mighty warriors to the viewless shades\n",
        "\n",
        "..... en tout 18334 lignes\n",
        "\n",
        "- [Samuel Butler](https://en.wikipedia.org/wiki/Samuel_Butler_%28novelist%29) — [text](https://storage.googleapis.com/download.tensorflow.org/data/illiad/butler.txt)\n",
        "\n",
        "Sing, O goddess, the anger of Achilles son of Peleus, that brought\n",
        "\n",
        "countless ills upon the Achaeans. Many a brave soul did it send\n",
        "\n",
        "hurrying down to Hades, and many a hero did it yield a prey to dogs and\n",
        "\n",
        "vultures, for so were the counsels of Jove fulfilled from the day on\n",
        "\n",
        ".... en tout 12131 lignes\n",
        "\n",
        "Les fichiers texte utilisés dans ce tutoriel ont subi quelques tâches de prétraitement typiques, la plupart du temps en supprimant des éléments - en-tête et pied de page du document, numéros de ligne, titres de chapitre. Téléchargez ces fichiers légèrement modifiés en local."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4YlKQthEYlFw",
        "outputId": "fe19bfc6-4395-43b2-f70d-81551c21ab6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "source": [
        "DIRECTORY_URL = 'https://storage.googleapis.com/download.tensorflow.org/data/illiad/'\n",
        "FILE_NAMES = ['cowper.txt', 'derby.txt', 'butler.txt']\n",
        "\n",
        "for name in FILE_NAMES:\n",
        "  text_dir = tf.keras.utils.get_file(name, origin=DIRECTORY_URL+name)\n",
        "  \n",
        "parent_dir = os.path.dirname(text_dir)\n",
        "\n",
        "parent_dir"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/illiad/cowper.txt\n",
            "819200/815980 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/illiad/derby.txt\n",
            "811008/809730 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/illiad/butler.txt\n",
            "811008/807992 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/root/.keras/datasets'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q3sDy6nuXoNp"
      },
      "source": [
        "## Charger le texte dans les ensembles de données\n",
        "\n",
        "Itérer les fichiers, en chargeant chacun d'entre eux dans son propre ensemble de données.\n",
        "\n",
        "Chaque exemple doit être étiqueté individuellement, donc utilisez `tf.data.Dataset.map` pour appliquer une fonction d'étiquetage à chacun. Cela va itérer sur chaque exemple de l'ensemble de données, en renvoyant des paires (`exemple, label`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "K0BjCOpOh7Ch",
        "colab": {}
      },
      "source": [
        "def labeler(example, index):\n",
        "  # Rappel, la fonction tf.cast permet de forcer le type d'index à un entier sur 64 bits\n",
        "  return example, tf.cast(index, tf.int64)  \n",
        "\n",
        "labeled_data_sets = []\n",
        "\n",
        "for i, file_name in enumerate(FILE_NAMES):\n",
        "  # Récupérer l'ensemble des lignes de file_name dans un objet lines_dataset de type tf.data.Dataset\n",
        "  lines_dataset = tf.data.TextLineDataset(os.path.join(parent_dir, file_name))\n",
        "  # Créer un mappage entre une ligne de line_dataset et le numéro de la traduction i\n",
        "  labeled_dataset = lines_dataset.map(lambda ex: labeler(ex, i))\n",
        "  # Ajouter labeled_dataset à labeled_data_sets pour n'obtenir en sortie qu'un seul tableau contenat les trois listes\n",
        "  labeled_data_sets.append(labeled_dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "M8PHK5J_cXE5"
      },
      "source": [
        "Combinez ces ensembles de données étiquetés en un seul ensemble de données, et mélangez le tout.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6jAeYkTIi9-2",
        "colab": {}
      },
      "source": [
        "# Taille du buffer utilisé lors du mélange des lignes, il est supérieur au nombre de lignes\n",
        "# afin de toutes les mélanger\n",
        "BUFFER_SIZE = 50000\n",
        "# Taille du lot à traiter qui correspond dans notre cas à fixer la longueur des lignes\n",
        "# Les lignes plus courtes seront complétées par des zéros\n",
        "BATCH_SIZE = 64\n",
        "# Nombre d'éléments de l'ensemble de test\n",
        "TAKE_SIZE = 5000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Qd544E-Sh63L",
        "colab": {}
      },
      "source": [
        "# Concatener les trois listes pour ne plus avoir dans all_labeled_data qu'une seule liste\n",
        "# contenant toutes les lignes des trois textes\n",
        "all_labeled_data = labeled_data_sets[0]\n",
        "for labeled_dataset in labeled_data_sets[1:]:\n",
        "  all_labeled_data = all_labeled_data.concatenate(labeled_dataset)\n",
        "# Mélanger les lignes dans la liste avec la fonction shuffle de l'objet tf.data.Dataset\n",
        "# le BUFFER_SIZE de 50000 permet de s'assurer que les 49608 lignes seront bien toutes mélangées\n",
        "all_labeled_data = all_labeled_data.shuffle(\n",
        "    BUFFER_SIZE, reshuffle_each_iteration=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r4JEHrJXeG5k"
      },
      "source": [
        "On peut utiliser `tf.data.Dataset.take` et `print` pour voir que les paires sont bien du type `(example, label)`. Les propriétés `numpy` montrent les valeurs de chaques Tenseurs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gywKlN0xh6u5",
        "outputId": "e8a1dc5c-d846-4c13-de01-fdfbaa4eb848",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "source": [
        "for ex in all_labeled_data.take(5):\n",
        "  print(ex)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(<tf.Tensor: shape=(), dtype=string, numpy=b'Olympus, and with such impetuous speed?'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
            "(<tf.Tensor: shape=(), dtype=string, numpy=b'The son of Tydeus was in two minds whether or no to turn his horses'>, <tf.Tensor: shape=(), dtype=int64, numpy=2>)\n",
            "(<tf.Tensor: shape=(), dtype=string, numpy=b'Thus was the head of Hector being dishonoured in the dust. His mother'>, <tf.Tensor: shape=(), dtype=int64, numpy=2>)\n",
            "(<tf.Tensor: shape=(), dtype=string, numpy=b'Prevailing, lured him with the bait of love.'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
            "(<tf.Tensor: shape=(), dtype=string, numpy=b'He said, and furious with his spear again'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5rrpU2_sfDh0"
      },
      "source": [
        "## Encoder les lignes de texte avec des nombres\n",
        "\n",
        "Les modèles d'apprentissage machine fonctionnent sur des nombres, et non sur des mots, de sorte que les valeurs des chaînes doivent être converties en listes de nombres. Pour ce faire, il faut faire correspondre chaque mot unique à un nombre entier unique.\n",
        "\n",
        "### Construire le vocabulaire\n",
        "\n",
        "Tout d'abord, créez un vocabulaire en transformant le texte en une collection de mots uniques. Il y a plusieurs façons de faire cela dans TensorFlow et Python. Pour ce tutoriel :\n",
        "\n",
        "1. Itérer sur la valeur \"numpy\" de chaque exemple.\n",
        "2. Utilisez `tfds.features.text.Tokenizer` pour la diviser en tokens (mots).\n",
        "3. Rassemblez ces tokens dans un ensemble Python, pour supprimer les doublons.\n",
        "4. Obtenez la taille du vocabulaire pour une utilisation ultérieure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YkHtbGnDh6mg",
        "outputId": "9e2a5106-a50f-49bb-c781-68ecfa8d704e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Le Tokenizer va nous permettre de rechercher les blocs de caractères (mots) dans les lignes\n",
        "tokenizer = tfds.features.text.Tokenizer()\n",
        "# Créer un ensemble (set)\n",
        "vocabulary_set = set()\n",
        "# Récupérer les lignes une à une dans all_labeled_data\n",
        "for text_tensor, _ in all_labeled_data:\n",
        "  # Dans chaque ligne, les tokens sont stockés dans some_tokens\n",
        "  some_tokens = tokenizer.tokenize(text_tensor.numpy())\n",
        "  # Avec la commande update sur un ensemble, on n'ajoute que les éléments qui n'étaient pas dans celle-ci \n",
        "  vocabulary_set.update(some_tokens)\n",
        "\n",
        "vocab_size = len(vocabulary_set)\n",
        "vocab_size"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17178"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0W35VJqAh9zs"
      },
      "source": [
        "### Encoder les exemples\n",
        "\n",
        "Créez un encodeur en passant le \"jeu de vocabulaire\" à `tfds.features.text.TokenTextEncoder`. La méthode `encode` de l'encodeur prend une chaîne de texte et retourne une liste d'entiers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gkxJIVAth6j0",
        "colab": {}
      },
      "source": [
        "encoder = tfds.features.text.TokenTextEncoder(vocabulary_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "v6S5Qyabi-vo"
      },
      "source": [
        "Vous pouvez l'essayer sur une seule ligne pour voir à quoi ressemble le résultat."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jgxPZaxUuTbk",
        "outputId": "022c715c-6984-4202-aae1-946af2729b7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "example_text = next(iter(all_labeled_data))[0].numpy()\n",
        "print(example_text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'Olympus, and with such impetuous speed?'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XoVpKR3qj5yb",
        "outputId": "501c55aa-26b5-499a-dab6-6412f4331df6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "encoded_example = encoder.encode(example_text)\n",
        "print(encoded_example)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[15371, 603, 9411, 6502, 4409, 10445]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "p9qHM0v8k_Mg"
      },
      "source": [
        "Maintenant, lancez l'encodeur sur le jeu de données en créant une fonction encode qui prend la ligne et son label en entrée et la ligne encodée et son label (qui ne change pas) en sortie."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HcIQ7LOTh6eT",
        "colab": {}
      },
      "source": [
        "def encode(text_tensor, label):\n",
        "  encoded_text = encoder.encode(text_tensor.numpy())\n",
        "  return encoded_text, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eES_Z1ia-Om-"
      },
      "source": [
        "Vous voulez utiliser `Dataset.map` pour appliquer cette fonction à chaque élément de l'ensemble de données.  Le fichier `Dataset.map` fonctionne en mode graphe.\n",
        "\n",
        "* Les tenseurs du graphe n'ont pas de valeur. \n",
        "* En mode graphe, vous ne pouvez utiliser que les opérations et les fonctions TensorFlow. \n",
        "\n",
        "Vous ne pouvez donc pas `.map` cette fonction directement : Vous devez l'intégrer dans une `tf.py_function`. La fonction `tf.py_function` passera les tensors réguliers (avec une valeur et une méthode `.numpy()` pour y accéder), à la fonction python enveloppée."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KmQVsAgJ-RM0",
        "colab": {}
      },
      "source": [
        "def encode_map_fn(text, label):\n",
        "  # py_function ne définit pas la forme des tenseurs retournés.\n",
        "  # tf.py_function prend en paramètres, le nom de la fonction, les entrées de celle-ci\n",
        "  # et le type des sorties au format tensorflow, ici des entiers tf.int64\n",
        "  encoded_text, label = tf.py_function(encode, \n",
        "                                       inp=[text, label], \n",
        "                                       Tout=(tf.int64, tf.int64))\n",
        "\n",
        "  # `tf.data.Datasets` fonctionne mieux si tous les composants ont des formes définies\n",
        "  #  on doit donc définir les formes manuellement: \n",
        "  encoded_text.set_shape([None])\n",
        "  label.set_shape([])\n",
        "\n",
        "  return encoded_text, label\n",
        "\n",
        "\n",
        "all_encoded_data = all_labeled_data.map(encode_map_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_YZToSXSm0qr"
      },
      "source": [
        "## Diviser l'ensemble de données en lots de test et d'apprentissage\n",
        "\n",
        "Utilisez `tf.data.Dataset.take` et `tf.data.Dataset.skip` pour créer un petit ensemble de données de test et un ensemble d'apprentissage plus important.\n",
        "\n",
        "Avant d'être passés dans le modèle, les ensembles de données doivent être mis en lots. En général, les exemples à l'intérieur d'un lot doivent avoir la même taille et la même forme. Mais les exemples de ces ensembles de données n'ont pas tous la même taille - chaque ligne de texte a un nombre de mots différent. Utilisez donc `tf.data.Dataset.padded_batch` (au lieu de `batch`) pour remplir les exemples avec la même taille."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "r-rmbijQh6bf",
        "colab": {}
      },
      "source": [
        "# skip(TAKE_SIZE) implique que l'on ne garde que les (49608 - 5000) lignes pour l'apprentissage\n",
        "# On remélange l'ensemble obtenu avec shuffle\n",
        "train_data = all_encoded_data.skip(TAKE_SIZE).shuffle(BUFFER_SIZE)\n",
        "# Toutes les lignes n'ont pas la même longueur, on remplit de réro la fin des lignes\n",
        "# jusqu'à obtenir BATCH_SIZE (64) éléments\n",
        "train_data = train_data.padded_batch(BATCH_SIZE, padded_shapes=([None],[]))\n",
        "# take(TAKE_SIZE) permet d'obtenir un ensemble de test de 5000 lignes\n",
        "test_data = all_encoded_data.take(TAKE_SIZE)\n",
        "test_data = test_data.padded_batch(BATCH_SIZE, padded_shapes=([None],[]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fFEHUueEVOxC"
      },
      "source": [
        "Note : A partir de **TensorFlow 2.2**, l'argument `padded_shapes` n'est plus nécessaire. Le comportement par défaut est de capitonner tous les axes au plus long du lot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0EMSOEPlVOYU",
        "colab": {}
      },
      "source": [
        "train_data = all_encoded_data.skip(TAKE_SIZE).shuffle(BUFFER_SIZE)\n",
        "train_data = train_data.padded_batch(BATCH_SIZE)\n",
        "\n",
        "test_data = all_encoded_data.take(TAKE_SIZE)\n",
        "test_data = test_data.padded_batch(BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Xdz7SVwmqi1l"
      },
      "source": [
        "Maintenant, `test_data` et `train_data` ne sont pas des collections de paires (`example, label`), mais des collections de lots. Chaque lot est une paire de (*many examples*, *many labels*) représentée comme des tableaux.\n",
        "\n",
        "Pour illustrer :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kMslWfuwoqpB",
        "colab": {}
      },
      "source": [
        "sample_text, sample_labels = next(iter(test_data))\n",
        "\n",
        "sample_text[0], sample_labels[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UI4I6_Sa0vWu"
      },
      "source": [
        "Depuis que nous avons introduit un nouveau codage de token (le zéro utilisé pour le padding), la taille du vocabulaire a augmenté de un."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IlD1Lli91vuc",
        "colab": {}
      },
      "source": [
        "vocab_size += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "K8SUhGFNsmRi"
      },
      "source": [
        "## Construire le modèle\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QJgI1pow2YR9",
        "colab": {}
      },
      "source": [
        "model = tf.keras.Sequential()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wi0iiKLTKdoF"
      },
      "source": [
        "La première couche convertit les représentations d'entiers en incrustations de vecteurs denses. Voir le [tutoriel sur les encastrements de mots](../text/word_embeddings.ipynb) ou plus de détails. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DR6-ctbY638P",
        "colab": {}
      },
      "source": [
        "# La couche Embedding permet de coder chaque élément du vocabulaire (valeur entière)\n",
        "# avec 64 réels de 0 à 1 qui sont comme des poids qui seront déterminés lors de l'apprentissage\n",
        "model.add(tf.keras.layers.Embedding(vocab_size, 64))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_8OJOPohKh1q"
      },
      "source": [
        "La couche suivante est une couche [Long Short-Term Memory] (http://colah.github.io/posts/2015-08-Understanding-LSTMs/), qui permet au modèle de comprendre les mots dans leur contexte avec d'autres mots. Un wrapper bidirectionnel sur le LSTM lui permet de connaître les points de données en relation avec les points de données qui l'ont précédé et suivi."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "x6rnq6DN_WUs",
        "colab": {}
      },
      "source": [
        "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cdffbMr5LF1g"
      },
      "source": [
        "Enfin, nous aurons une série d'une ou plusieurs couches densément connectées, la dernière étant la couche de sortie. La couche de sortie produit une probabilité pour tous les labels. Celui qui a la plus forte probabilité est la prédiction des modèles du label d'un exemple."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QTEaNSnLCsv5",
        "colab": {}
      },
      "source": [
        "# Une ou plusieurs couches denses.\n",
        "# Editer la liste à la ligne `for` pour tester les tailles des couches.\n",
        "for units in [64, 64]:\n",
        "  model.add(tf.keras.layers.Dense(units, activation='relu'))\n",
        "\n",
        "# Couche de sortie. L'argume,nt est le nombre de labels.\n",
        "model.add(tf.keras.layers.Dense(3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zLHPU8q5DLi_"
      },
      "source": [
        "Enfin, compilez le modèle. Pour un modèle de catégorisation softmax, utilisez `sparse_categorical_crossentropy` comme fonction de perte. Vous pouvez essayer d'autres optimiseurs, mais `adam` est très courant."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pkTBUVO4h6Y5",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DM-HLo5NDhql"
      },
      "source": [
        "## Former le modèle\n",
        "\n",
        "Ce modèle fonctionnant sur ces données donne des résultats décents (environ 83% avec 3 epochs). Nous allons améliorer ce résultat en passant à 10 epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aLtO33tNh6V8",
        "colab": {}
      },
      "source": [
        "model.fit(train_data, epochs=10, validation_data=test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KTPCYf_Jh6TH",
        "colab": {}
      },
      "source": [
        "eval_loss, eval_acc = model.evaluate(test_data)\n",
        "\n",
        "print('\\nEval loss: {:.3f}, Eval accuracy: {:.3f}'.format(eval_loss, eval_acc))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}